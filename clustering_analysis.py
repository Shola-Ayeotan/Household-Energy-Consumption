# -*- coding: utf-8 -*-
"""Clustering Analysis  - Shola Ayeotan

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k2AZwGW150JRl1rOlQbiMdWV6k8iBXdg
"""

# Importing the necessary libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from pandas.core.common import random_state
from numpy import mean, std

"""#### **Loading the Dataset**"""

# Reading in the CSV file

power_consumption = pd.read_csv('/content/household_power_consumption.txt', delimiter=';',
                               parse_dates={'TimeStamp' : ['Date', 'Time']},
                                infer_datetime_format=True, low_memory=False, index_col='TimeStamp')

# # Reading in the CSV file

# power_consumption = pd.read_csv('household_power_consumption.txt', delimiter=';',
#                                parse_dates={'TimeStamp' : ['Date', 'Time']},
#                                 infer_datetime_format=True, low_memory=False, index_col='TimeStamp')

power_consumption.shape

power_consumption.head()

"""##### **Sampling a subset of the data**
Taking a random sample from the population.
"""

# Sampling 10,000 records randomly

power_sample = power_consumption.sample(n=5000, random_state=42)

power_sample.head()

power_sample.tail()

power_sample.info()

"""#### **Data Cleaning and Preprocessing**"""

# Checking for null values

power_sample.isnull().sum()

## Some of the values in certain columns are represented by '?'

power_sample = power_sample.replace('?', np.nan)

power_sample.isnull().sum()

power_sample = power_sample.astype(np.float64).fillna(method='bfill')

power_sample.isnull().sum()

power_sample.info()

# Renaming the sub_metering columns to household usage units

power_sample.rename(columns={
    'Sub_metering_1': 'Kitchen_Use',
    'Sub_metering_2': 'Laundry_Use',
    'Sub_metering_3': 'Heating_Use'}, inplace=True)

# Deriving a column for energy readings unaccounted for

power_sample['Unaccounted_Use'] = power_sample['Global_active_power'] * 1000 / 60 - \
                                              power_sample['Kitchen_Use'] - power_sample['Laundry_Use'] - power_sample['Heating_Use']

# Combining all four energy readings to derive a column for total usage

power_sample['Total_Household_Consumption'] = power_sample['Kitchen_Use'] + \
                                               power_sample['Laundry_Use'] + \
                                               power_sample['Heating_Use'] + \
                                               power_sample['Unaccounted_Use']

# Deriving variables for different time intervals

power_sample['hour_of_day'] = power_sample.index.hour

power_sample['day_of_week'] = power_sample.index.dayofweek

power_sample['month_of_year'] = power_sample.index.month

power_sample['season'] = power_sample.index.month % 12 // 3

power_sample.info()

"""#### Outlier Detection and Removal"""

# Checking for outliers

plt.figure(figsize=(22, 10))
power_sample.boxplot();

# # calculating the quartiles
# Q1 = power_sample.quantile(0.25)

# Q3 = power_sample.quantile(0.75)

# IQR = Q3 - Q1

# power_sample = power_sample[~((power_sample < (Q1 - 1.5 * IQR)) | (power_sample > (Q3 + 1.5 * IQR))).any(axis=1)]

# Creating a list to store outlier percentages for each column
outlier_percentages = []

numerical_features = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity',
                     'Kitchen_Use', 'Laundry_Use', 'Heating_Use', 'Unaccounted_Use', 'Total_Household_Consumption']


# Iterating over each column
for column in numerical_features:
    data = power_sample[column]

    # Calculating mean and standard deviation
    column_mean, column_std = mean(data), std(data)

    # Setting threshold for identifying outliers
    cut_off = column_std * 3
    lower, upper = column_mean - cut_off, column_mean + cut_off

    print(f'{column}: \n')

    # Identifying and removing the outliers
    outliers = data[(data < lower) | (data > upper)]
    power_sample = power_sample[(data >= lower) & (data <= upper)]

    # Counting and printing identified outliers
    num_out = len(outliers)
    print(f'Identified outliers: {num_out}')

    # Removing the outliers and counting non-outlier observations
    outliers_removed = data[(data >= lower) & (data <= upper)]
    num_nout = len(outliers_removed)
    print(f'Non-outlier observations: {num_nout}')

    # Calculating and printing percentage of outliers
    outlier_percent = (num_out / (num_out + num_nout)) * 100
    print('Percent of outliers:', outlier_percent, '\n')

    # Storing outlier percentage for the current column
    outlier_percentages.append(outlier_percent)

# Visualizing the identified outliers, their columns and percentages

plt.figure(figsize=(8, 5))

outliers = pd.DataFrame({'Feature': numerical_features, '% Of Outliers': outlier_percentages})

sns.barplot(data=outliers.sort_values('% Of Outliers', ascending=False), y='Feature', x='% Of Outliers', palette='GnBu_d')

plt.title('Percent of Outliers by columns')
plt.ylabel('Column')
plt.show()

"""#### **Exploratory Analysis and Visualization**"""

power_sample.describe()

# Visualizing the contribution of different readings to Total Household Energy Consumption

fig, ax = plt.subplots(figsize=(15, 6))

monthly_consumption = power_sample[['Kitchen_Use', 'Laundry_Use', 'Heating_Use', 'Unaccounted_Use' ]].resample('M').sum()
monthly_consumption.index = monthly_consumption.index.strftime('%b %Y')
monthly_consumption.plot(kind='bar', stacked=True, ax=ax)

plt.title('Monthly Household Energy Consumption Breakdown')
plt.xlabel('Month')
plt.ylabel('Energy Consumption (Watt-hour)')
plt.xticks(rotation=45, ha='right')
plt.show()

# Resampling based on seasonal interval and summing up the energy readings

seasonal_consumption = power_sample[['Kitchen_Use', 'Laundry_Use', 'Heating_Use', 'Unaccounted_Use', 'Total_Household_Consumption']].resample('Q').sum()

percentage_contributions_seasonal = seasonal_consumption[['Kitchen_Use', 'Laundry_Use', 'Heating_Use', 'Unaccounted_Use']].\
div(seasonal_consumption['Total_Household_Consumption'], axis=0) * 100

fig, ax = plt.subplots(figsize=(12, 6))
percentage_contributions_seasonal.plot(kind='bar', stacked=True, ax=ax)

season_names = {1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Autumn'}
season_labels = [f'{season_names[quarter]} {year}' for quarter, year in zip(seasonal_consumption.index.quarter, seasonal_consumption.index.year)]

plt.title('Proportion of Energy Consumption from Different Appliances - Seasonal Breakdown')
plt.xlabel('Season')
plt.ylabel('Percentage Contribution')
plt.xticks(range(len(seasonal_consumption)), season_labels, rotation=45, ha='right')
plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))
plt.show()

# Plotting consumption over time

plt.figure(figsize=(15, 6))
power_sample['Total_Household_Consumption'].plot(title='Household Consumption Pattern')
plt.xlabel('Time')
plt.ylabel('Household Consumption (kW)')
plt.show()

# Resampling using different time intervals
daily_pattern = power_sample['Total_Household_Consumption'].resample('D').mean()
weekly_pattern = power_sample['Total_Household_Consumption'].resample('W').mean()
monthly_pattern = power_sample['Total_Household_Consumption'].resample('M').mean()
seasonal_pattern = power_sample['Total_Household_Consumption'].resample('Q').mean()


# Plotting all patterns together
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
daily_pattern.plot(title='Daily Household Consumption Pattern', label='Daily', color='blue')
plt.xlabel('Date')
plt.ylabel('Average Daily Consumption (kW)')
plt.legend()

plt.subplot(2, 2, 2)
weekly_pattern.plot(title='Weekly Household Consumption Pattern', label='Weekly', color='orange')
plt.xlabel('Week')
plt.ylabel('Average Weekly Consumption (kW)')
plt.legend()

plt.subplot(2, 2, 3)
monthly_pattern.plot(title='Monthly Household Consumption Pattern', label='Monthly', color='green')
plt.xlabel('Month')
plt.ylabel('Average Monthly Consumption (kW)')
plt.legend()

plt.subplot(2, 2, 4)
seasonal_pattern.plot(title='Seasonal Household Consumption Pattern', label='Seasonal', color='red')
plt.xlabel('Quarter')
plt.ylabel('Average Quarterly Consumption (kW)')
plt.legend()

plt.tight_layout()
plt.show()

# Selecting variables
distribution_vars = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity']

# Plotting distributions
plt.figure(figsize=(15, 10))
for i, var in enumerate(distribution_vars, 1):
    plt.subplot(2, 2, i)
    sns.histplot(power_sample[var], kde=True)
    plt.title(f'Distribution of {var}')
    plt.xlabel(var)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Visualzing mean of 'Voltage' resampled over months

plt.figure(figsize=(10, 6))
power_sample['Voltage'].resample('M').mean().plot(kind='bar', color='red')

plt.xticks(rotation=60)
plt.ylabel('Voltage')
plt.title('Monthly Average Voltage')
plt.show()

"""##### **Observation**:
It can be observed from the above plot that the mean of 'Voltage' over monthly periods is pretty much constant compared to other features.

#### **Correlation Analysis**
"""

# Plotting the correlation heatmap

correlation_variables = power_sample.iloc[:, 0:9]

correlation_matrix = correlation_variables.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap')
plt.show()

sns.pairplot(power_sample.iloc[:, [0,1,3]])

# Exploring the relationship between sub-metering readings and total household consumption

plt.figure(figsize=(16, 8))

plt.subplot(2, 2, 1)
sns.scatterplot(x='Kitchen_Use', y='Total_Household_Consumption', data=power_sample)
plt.title('Kitchen Use vs Total Consumption')

plt.subplot(2, 2, 2)
sns.scatterplot(x='Laundry_Use', y='Total_Household_Consumption', data=power_sample)
plt.title('Laundry Use vs Total Consumption')

plt.subplot(2, 2, 3)
sns.scatterplot(x='Heating_Use', y='Total_Household_Consumption', data=power_sample)
plt.title('Heating Use vs Total Consumption')

plt.subplot(2, 2, 4)
sns.scatterplot(x='Unaccounted_Use', y='Total_Household_Consumption', data=power_sample)
plt.title('Unaccounted Use vs Total Consumption')

plt.tight_layout()
plt.show()

"""#### **Standardizing the Dataset**"""

power_sample.info()

# Selecting relevant features for clustering
selected_features = ['Global_active_power',
    'Global_reactive_power',
    'Global_intensity',
    # 'Voltage',
    'Kitchen_Use',
    'Laundry_Use',
    'Heating_Use',
    'Unaccounted_Use']

X = power_sample[selected_features]

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X = scaler.fit_transform(X)

"""#### **KMeans Algorithm**"""

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Using the elbow method to find the optimal number of clusters

from sklearn.cluster import KMeans

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss, marker='o')
plt.title('The Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-Cluster-Sum-of-Squares)')

plt.show()

# Reducing the dimensionality in the dataset

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

pca.explained_variance_ratio_

sum(pca.explained_variance_ratio_)

PC_values = np.arange(pca.n_components_) + 1
plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.show()

# Fitting K-Means to the dataset
kmeans = KMeans(n_clusters = 5, init= 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X_pca)

# Adding a 'Clusters' column to the main dataframe
power_sample['Clusters'] = y_kmeans + 1

# Visualizing the clusterings

plt.figure(figsize=(8, 6))

sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=power_sample['Clusters'], palette='viridis', s=70)

centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, color='red', label='Centroids')

plt.title('Clustering Results with Centroids')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()

colors = ['red', 'blue', 'green', 'orange', 'black']

plt.figure(figsize=(8, 6))

for i in range(5):
    plt.scatter(X_pca[y_kmeans == i, 0], X_pca[y_kmeans == i, 1],
                s=100, c=colors[i], label='Cluster ' + str(i + 1))

plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=200, c='yellow', marker='X', label='Centroids')

plt.title('Clusters of Electrical Usage with Centroids')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()

"""##### Evaluating the model

Silhouette Score
"""

from sklearn.metrics import silhouette_score, silhouette_samples

labels = kmeans.labels_
silhouette_value = silhouette_score(X_pca, labels)
print(f"The Silhouette Score is: {silhouette_value}")

from sklearn.metrics import davies_bouldin_score

# Calculating the Davies-Bouldin Index

labels = kmeans.labels_

labelstable = pd.DataFrame(labels)

db_index = davies_bouldin_score(X_pca, labels)
print(f"Davies-Bouldin Index: {db_index}")

"""#### Hierarchical Clustering"""

from sklearn.cluster import AgglomerativeClustering
from seaborn.matrix import dendrogram
import scipy.cluster.hierarchy as sch

plt.figure(figsize=(12, 8))

dendrogram = sch.dendrogram(sch.linkage(X_pca, method = 'ward'))

plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Data Points')
plt.ylabel('Euclidean Distance')
plt.show()

# Fitting the hierachichal clustering to the dataset

from sklearn.cluster import AgglomerativeClustering

hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')

y_hc = hc.fit_predict(X_pca)

colors = ['red', 'blue', 'green', 'orange', 'skyblue']

plt.figure(figsize=(12, 8))

for i in range(5):
    plt.scatter(X_pca[y_hc == i, 0], X_pca[y_hc == i, 1],
                s=100, c=colors[i], label='Cluster ' + str(i + 1))

plt.title('Clusters of Electrical Usage with Centroids')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()

"""##### **Davies-Bouldin Index**"""

model = AgglomerativeClustering(n_clusters=5,affinity="euclidean",linkage="ward")

labels = model.fit_predict(X_pca)

db_index = davies_bouldin_score(X_pca, labels)
print(f"Davies-Bouldin Index: {db_index}")

# model = AgglomerativeClustering(n_clusters=5,affinity="euclidean",linkage="ward")

# labels = model.fit_predict(X_pca)

# labelstable = pd.DataFrame(labels)

# db_index = davies_bouldin_score(X_pca, labels)
# print(f"Davies-Bouldin Index: {db_index}")

"""### **Clusters Interpretation**

"""

power_sample

pl = sns.countplot(x=power_sample["Clusters"])

pl.set_title("Distribution Of The Clusters")
plt.show()

# Visualizing the Cluster Profiles by their average values
cluster_means = power_sample.drop(columns=['hour_of_day', 'day_of_week', 'month_of_year', 'season'], axis=1).groupby('Clusters').mean()

plt.figure(figsize=(12, 8))
sns.heatmap(cluster_means.T, annot=True, cmap='viridis', fmt='.2f')
plt.title('Cluster Profiling: Average Feature Values')
plt.xlabel('Cluster')
plt.ylabel('Feature')
plt.show()

# Visualizing the average consumption per season for each cluster
seasonal_means = power_sample.groupby(['Clusters', 'season']).mean()

seasonal_means['Global_active_power'].unstack().plot(kind='bar', figsize=(12, 6))
plt.title('Seasonal Average Power Consumption for Each Cluster')
plt.ylabel('Average Power Consumption')
plt.xlabel('Season')
plt.show()

# Visualize the average consumption per hour for each cluster

hourly_means = power_sample.groupby(['hour_of_day', 'Clusters']).mean()

hourly_means['Global_active_power'].unstack().plot(kind='line', figsize=(12, 6))

plt.title('Hourly Average Power Consumption for Each Cluster')
plt.ylabel('Average Power Consumption')
plt.xlabel('Hour of Day')

plt.show()

